% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/apply.R
\name{spark_udf}
\alias{spark_udf}
\title{Apply an R UDF in Spark}
\usage{
spark_udf(.data, .f, schema)
}
\arguments{
\item{.data}{a \code{spark_tbl}}

\item{.f}{a function or formula to be applied to each partition of the
\code{spark_tbl}. Can be an anonymous function e.g. \code{~ head(. 10)}
\code{.f} should have only one parameter, to which a R data.frame corresponds
to each partition will be passed. The output of func should be an R data.frame.}

\item{schema}{The schema of the resulting SparkDataFrame after the function
is applied. It must match the output of func. Since Spark 2.3, the
DDL-formatted string is also supported for the schema.}
}
\value{
a \code{spark_tbl}
}
\description{
Applies an R User-Defined Function (UDF) to each partition of
a \code{spark_tbl}.
}
\details{
\code{spark_udf} is a re-implementation of \code{SparkR::dapply}.
Importantly, \code{spark_udf} (and \code{SparkR::dapply}) will scan the
function being passed and automatically broadcast any values from the
\code{.GlobalEnv} that are being referenced. Functions from \code{dplyr} are
always availiable by default.
}
\examples{

\dontrun{
iris_tbl <- spark_tbl(iris)

# note, my_var will be broadcasted if we include it in the function
my_var <- 1

iris_tbl \%>\%
  spark_udf(function(.df) head(.df, my_var),
            schema(iris_tbl)) \%>\%
  collect

# but if you want to use a library (other than dplyr), you need to load it
# in the UDF
iris_tbl \%>\%
  spark_udf(function(.df) {
    require(purrr)
    .df \%>\%
      map_df(first)
  }, schema(iris_tbl)) \%>\%
  collect

# filter and add a column:
df <- spark_tbl(
  data.frame(a = c(1L, 2L, 3L),
             b = c(1, 2, 3),
             c = c("1","2","3"))
)

schema <- StructType(StructField("a", "integer"),
                     StructField("b", "double"),
                     StructField("c", "string"),
                     StructField("add", "integer"))

df \%>\%
  spark_udf(function(x) {
    x \%>\%
      filter(a > 1) \%>\%
      mutate(add = a + 1L)
  },
  schema) \%>\%
  collect

# The schema also can be specified in a DDL-formatted string.
schema <- "a INT, d DOUBLE, c STRING, add INT"
df \%>\%
  spark_udf(function(x) {
    x \%>\%
      filter(a > 1) \%>\%
      mutate(add = a + 1L)
  },
  schema) \%>\%
  collect
}
}
