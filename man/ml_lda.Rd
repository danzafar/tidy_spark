% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_clustering.R
\name{ml_lda}
\alias{ml_lda}
\alias{summary,LDAModel-method}
\alias{ml_perplexity}
\alias{ml_perplexity,LDAModel-method}
\alias{ml_posterior}
\alias{ml_posterior,LDAModel,spark_tbl-method}
\alias{write_ml,LDAModel,character-method}
\title{Latent Dirichlet Allocation}
\usage{
ml_lda(
  data,
  features = "features",
  k = 10,
  maxIter = 20,
  optimizer = c("online", "em"),
  subsamplingRate = 0.05,
  topicConcentration = -1,
  docConcentration = -1,
  customizedStopWords = "",
  maxVocabSize = bitwShiftL(1, 18)
)

\S4method{summary}{LDAModel}(object, maxTermsPerTopic)

ml_perplexity(object, data)

ml_posterior(object, newData)

\S4method{write_ml}{LDAModel,character}(object, path, overwrite = FALSE)
}
\arguments{
\item{data}{A spark_tbl for training.}

\item{features}{Features column name. Either libSVM-format column or
character-format column is valid.}

\item{k}{Number of topics.}

\item{maxIter}{Maximum iterations.}

\item{optimizer}{Optimizer to train an LDA model, "online" or "em", default
is "online".}

\item{subsamplingRate}{(For online optimizer) Fraction of the corpus to be
sampled and used in each iteration of mini-batch gradient descent,
in range (0, 1].}

\item{topicConcentration}{concentration parameter (commonly named \code{beta}
or \code{eta}) for the prior placed on topic distributions over terms,
default -1 to set automatically on the Spark side. Use \code{summary}
to retrieve the effective topicConcentration. Only 1-size numeric is
accepted.}

\item{docConcentration}{concentration parameter (commonly named \code{alpha})
for the prior placed on documents distributions over topics
(\code{theta}), default -1 to set automatically on the Spark side.
Use \code{summary} to retrieve the effective docConcentration. Only
1-size or \code{k}-size numeric is accepted.}

\item{customizedStopWords}{stopwords that need to be removed from the given
corpus. Ignore the parameter if libSVM-format column is used as the
features column.}

\item{maxVocabSize}{maximum vocabulary size, default 1 << 18}

\item{object}{A Latent Dirichlet Allocation model fitted by \code{spark.lda}.}

\item{maxTermsPerTopic}{Maximum number of terms to collect for each topic. Default value of 10.}

\item{newData}{A spark_tbl for testing.}

\item{path}{The directory where the model is saved.}

\item{overwrite}{Overwrites or not if the output path already exists. Default is FALSE
which means throw exception if the output path exists.}

\item{...}{additional argument(s) passed to the method.}
}
\value{
\code{ml_lda} returns a fitted Latent Dirichlet Allocation model.

\code{summary} returns summary information of the fitted model, which is a list.
        The list includes
        \item{\code{docConcentration}}{concentration parameter commonly named \code{alpha} for
              the prior placed on documents distributions over topics \code{theta}}
        \item{\code{topicConcentration}}{concentration parameter commonly named \code{beta} or
              \code{eta} for the prior placed on topic distributions over terms}
        \item{\code{logLikelihood}}{log likelihood of the entire corpus}
        \item{\code{logPerplexity}}{log perplexity}
        \item{\code{isDistributed}}{TRUE for distributed model while FALSE for local model}
        \item{\code{vocabSize}}{number of terms in the corpus}
        \item{\code{topics}}{top 10 terms and their weights of all topics}
        \item{\code{vocabulary}}{whole terms of the training corpus, NULL if libsvm format file
              used as training set}
        \item{\code{trainingLogLikelihood}}{Log likelihood of the observed tokens in the
              training set, given the current parameter estimates:
              log P(docs | topics, topic distributions for docs, Dirichlet hyperparameters)
              It is only for distributed LDA model (i.e., optimizer = "em")}
        \item{\code{logPrior}}{Log probability of the current parameter estimate:
              log P(topics, topic distributions for docs | Dirichlet hyperparameters)
              It is only for distributed LDA model (i.e., optimizer = "em")}

\code{ml_perplexity} returns the log perplexity of given
        spark_tbl, or the log perplexity of the training data if
        missing argument "data".

\code{ml_posterior} returns a spark_tbl containing posterior probabilities
        vectors named "topicDistribution".
}
\description{
\code{ml_lda} fits a Latent Dirichlet Allocation model on a spark_tbl.
Users can call
\code{summary} to get a summary of the fitted LDA model.
}
\note{
summary(LDAModel) since 2.1.0

ml_perplexity(LDAModel) since 2.1.0

ml_posterior(LDAModel) since 2.1.0

write_ml(LDAModel, character) since 2.1.0
}
\seealso{
topicmodels: \url{https://cran.r-project.org/package=topicmodels}

\link{read_ml}
}
