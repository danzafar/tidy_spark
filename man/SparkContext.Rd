% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sparkContext.R
\docType{data}
\name{SparkContext}
\alias{SparkContext}
\title{The \code{SparkContext} Class}
\format{An object of class \code{R6ClassGenerator} of length 24.}
\usage{
SparkContext
}
\arguments{
\item{sc}{optional, can instatiate with another SparkContext's jobj.}

\item{path}{string}

\item{recursive}{boolean
Add Jar}

\item{path}{string
App Name}

\item{value}{the variable to broadcast.}

\item{groupId}{string
clearJobGroup}

\item{seq}{list (or Scala Collection) to distribute}

\item{numSlices}{number of partitions to divide the collection into}

\item{directory}{string, path to the directory where checkpoint files will
be stored (must be HDFS path if running in cluster)}

\item{value}{string}

\item{groupId}{string}

\item{description}{string}

\item{interruptOnCancel}{If TRUE, then job cancellation will result in
Thread.interrupt() being called on the job's executor threads. This is
useful to help ensure that the tasks are actually stopped in a timely
manner, but is off by default due to HDFS-1208, where HDFS may respond to
Thread.interrupt() by marking nodes as dead.
setLocalProperty}

\item{key}{string}

\item{value}{string}

\item{path}{string, path to the text file on a supported file system}

\item{minPartitions}{int, suggested minimum number of partitions for the
resulting RDD}

\item{rdds}{a list of RDDs or RDD jobjs}

\item{path}{Directory to the input data files, the path can be comma
separated paths as the list of inputs.}

\item{minPartitions}{A suggestion value of the minimal splitting number
for input data.}
}
\value{
RDD
isLocal

boolean
jars

a jobj representing \code{scala.collection.Seq<String>}
master

string
Parallelize

RDD
setCheckpointDir

RDD
wholeTextFiles

RDD
}
\description{
This class was designed as a thin wrapper around Spark's
\code{SparkContext}. It is initialized when \code{spark_submit} is called
and inserted into the workspace as \code{sc}. Note, running
\code{sc$stop} will end your session. For information on methods and types
requirements, refer to the \href{https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html}{javadoc}:

Create a new \code{SparkContext}

print \code{SparkContext}
Add File

Add a file to be downloaded with this Spark job on every node.

Adds a JAR dependency for all tasks to be executed on this SparkContext in
the future.

get the App name
Broadcast

Broadcast a vairable to executors.
cancelAllJobs

Cancel all jobs that have been scheduled or are running.
cancelJobGroup

Cancel active jobs for the specified group.

Clear the current thread's job group ID and its description.
defaultMinPartitions

Default min number of partitions for Hadoop RDDs when not given by user
Notice that we use math.min so the "defaultMinPartitions" cannot be higher
than 2.
defaultParallelism

Default level of parallelism to use when not given by user
emptyRDD

Get an RDD that has no partitions or elements.

is the Spark process local?

is the Spark process local?

why is roxygen making me do all these...

Distribute a list (or Scala collection) to form an RDD.

Set the directory under which RDDs are going to be checkpointed.
setJobDescription

Set a human readable description of the current job.
setJobGroup

Assigns a group ID to all the jobs started by this thread until the
group ID is set to a different value or cleared.

Set a local property that affects jobs submitted from this thread, such
as the Spark fair scheduler pool.
sparkuser

Who AM I?
startTime

still surprised I have to write these. but the big bad orange
warnings that roxygen throws are just sooooo ugly
stop

Shut down the SparkContext.
textFile

Read a text file from HDFS, a local file system (available
on all nodes), or any Hadoop-supported file system URI, and return it as
an RDD of Strings.
version

The version of Spark on which this application is running.
Union RDDs

Build the union of a list of RDDs.

Read a directory of text files from HDFS, a local file system
(available on all nodes), or any Hadoop-supported file system URI.
}
\details{
Not all methods are implemented due to compatability
and tidyspark best practice usage conflicts. If you need to use a method not
included, try calling it using \code{call_method(sc$jobj, <yourMethod>)}.

Parallelize acts lazily. If seq is a mutable collection and is
altered after the call to parallelize and before the first action on the
RDD, the resultant RDD will reflect the modified collection. Pass a copy
of the argument to avoid this., avoid using parallelize(Seq()) to create
an empty RDD. Consider emptyRDD for an RDD with no partitions, or
parallelize(Seq[T]()) for an RDD of T with empty partitions.
}
\section{Fields}{

\describe{
\item{\code{jobj}}{\code{SparkContext} java object}

\item{\code{getConf}}{get the \code{SparkConf}}
}}

\examples{
\dontrun{
spark <- spark_session()
sc <- spark$sparkContext
sc$defaultParallelism()
an_rdd <- sc$parallelize(list(1:10), 4)
sc$getConf$get("spark.submit.deployMode")

spark_session_stop()
}
}
\keyword{datasets}
