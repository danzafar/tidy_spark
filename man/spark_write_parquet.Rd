% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/read-write.R
\name{spark_write_parquet}
\alias{spark_write_parquet}
\title{Write a \code{spark_tbl} to Parquet format}
\usage{
spark_write_parquet(.data, path, mode = "error", partition_by = NULL,
  ...)
}
\arguments{
\item{.data}{a \code{spark_tbl}}

\item{path}{string, the path where the file is to be saved.}

\item{mode}{string, usually \code{"error"} (default), \code{"overwrite"},
\code{"append"}, or \code{"ignore"}}

\item{partition_by}{string, column names to partition by on disk}

\item{...}{any other named options. See details below.}
}
\description{
Write a \code{spark_tbl} to a parquet file.
}
\details{
For Parquet, compression can be set using \code{...}. Compression
(default is the value specified in spark.sql.orc.compression.codec):
compression codec to use when saving to file. This can be one of the known
case-insensitive shorten names(none, snappy, zlib, and lzo).. More
information can be found here:
https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameWriter.html#parquet-java.lang.String-
}
