% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/read-write.R
\name{spark_read_delta}
\alias{spark_read_delta}
\title{Read a Delta file into a \code{spark_tbl}.}
\usage{
spark_read_delta(path, version = NULL, timestamp = NULL, ...)
}
\arguments{
\item{path}{string, the path to the file. Needs to be accessible from the cluster.}

\item{version}{numeric, the version of the Delta table. Can be obtained from
the output of DESCRIBE HISTORY events. Alias of \code{timestampAsOf}.}

\item{timestamp}{string, the time-based version of the Delta table to pull.
Only date or timestamp strings are accepted. For example, "2019-01-01" and
"2019-01-01T00:00:00.000Z". Alias of \code{versionAsOf}.}

\item{...}{optional named arguments to the reader.}
}
\value{
a \code{spark_tbl}
}
\description{
Read a Delta file into a \code{spark_tbl}.
}
\details{
Other options such as specifing a schema can be specified in the \code{...}
For more information on \code{version} and \code{timestamp}, see
https://docs.databricks.com/delta/delta-batch.html#dataframereader-options
}
\examples{
\dontrun{
spark_session(sparkPackages = "io.delta:delta-core_2.11:0.5.0")

iris_tbl <- spark_tbl(iris)

iris_tbl \%>\%
  spark_write_delta("/tmp/iris_tbl")

spark_read_delta("/tmp/iris_tbl") \%>\%
  collect
}
}
